---
title: "Utilizing RAPTOR Score to Predict NBA Playoff Performance"
output: html_document
date: "2024-11-07"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
```

# Introduction

This project leverages data from FiveThirtyEight’s RAPTOR score dataset, a modern metric designed to evaluate the efficacy of NBA players. Unlike older metrics such as CARMELO and DRAYMOND, RAPTOR incorporates advanced player-tracking and play-by-play data, emphasizing players' contributions to modern NBA schemes. Key attributes of this metric include its ability to account for shot creation, floor spacing, and defensive intensity, offering a more comprehensive evaluation of a player's impact on their team.

In addition to RAPTOR scores, the prepared dataset integrates team-level performance data, including overall season records, sourced from Basketball Reference. This combination of player- and team-level metrics allows for a detailed exploration of factors influencing playoff success, defined by the furthest round a team achieves in the postseason. 

The primary objective of this analysis is to develop predictive models to forecast playoff performance of NBA teams. This question is pertinent as much of the existing literature is concerned with predicting the outcome of an individual game - not postseason success. By addressing this question, the project aims to provide insights into the relationship between advanced player metrics and team success, with implications for team-building strategies and performance forecasting in professional basketball.

![Stephen Curry](https://media.cnn.com/api/v1/images/stellar/prod/2024-04-10t054204z-1645757524-mt1usatoday22988403-rtrmadp-3-nba-golden-state-warriors-at-los-angeles-lakers.JPG?c=16x9&q=h_833,w_1480,c_fill)

# Literature review of existing methods 

Matthew Houde created a baseline model where Gaussian Naïve Bayes was evaluated as the strongest model of predicting whether a team wins a given matchup. This is based on a variety of team aggregate stats including win percentage, field goal percentage, and 3-point shooting percentage (Houde). This yielded a 65% accuracy score.

Jake Kandell conducted a unique method where a z score for ELO was calculated for each team, then a logistic model was applied to the calculated z score and other statistics including win percentage, field goal percentage, and 3-point shooting percentage. This method yielded an astounding 70% accuracy. 

Although this method is effective, its implementation is incredibly sophisticated and outside the scope of the class. Thus, for the sake of simplicity, a z-score approach was not used for this analysis.

# Data collection

The RAPTOR dataset is retrieved from 538's RAPTOR dataset (insert link).
The remaining data was retrieved from BasketballReference, which is a database of various basketball data including wins, true shooting percentage, and post-season record.

### Data Manipulation and Merging

We start by loading in all the playoff performance data, binding them together, and changing ranks to actual outcomes
```{r}
#Reading in rank files
file_list <- c("data_2014.csv", "data_2015.csv", "data_2016.csv", "data_2017.csv", 
               "data_2018.csv", "data_2019.csv", "data_2020.csv", "data_2021.csv")
years <- c(2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021)  #years vector
modified_data_list <- list()
for (i in 1:length(file_list)) {
  data <- read.csv(file.path('./nbacsv', file_list[i]), header = FALSE)
  
  if(ncol(data) != 2) { # in case data is missing values
    print("Error detected in file" + file_list[i] + "!")
  }
  
  data$Year <- years[i]
  colnames(data) <- c("Result", "Name", "Year")
  modified_data_list[[i]] <- data
}
playoff_data <- do.call(rbind, modified_data_list)

# changing rank names
playoff_data <- playoff_data |>
  mutate(Result = case_when(
    Result == 1 | Result == 2 ~ "Final",
    Result == 3 | Result == 4 ~ "Conference Final",
    Result >= 5 & Result <= 8 ~ "Round 2",
    Result >= 9 & Result <= 16 ~ "Round 1"
  ))

head(playoff_data)
```

Reading in regular season stats
```{r}
# reading in win files
win_list <- c("wins_2014.csv", "wins_2015.csv", "wins_2016.csv", "wins_2017.csv", 
              "wins_2018.csv", "wins_2019.csv", "wins_2020.csv", "wins_2021.csv")
years <- c(2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021)
modified_win_list <- list()
for (i in 1:length(win_list)) {
  windata <- read.csv(file.path('nbacsv', win_list[i]), header = FALSE)
  if (ncol(windata) == 2) {
    windata$Year <- years[i]
    colnames(windata) <- c("Name", "Wins", "Year")
    modified_win_list[[i]] <- windata
  }
}

# combine files
combined_wins <- do.call(rbind, modified_win_list)
#removing wins that don't make it to playoffs, which are teams without an *
combined_wins <- combined_wins |>
  filter(grepl("\\*", Name)) |>
  mutate(Name = gsub("\\*$", "", Name))

head(combined_wins)
```

We also want true shooting percentage, which will be another predictor we will use.
```{r}
# Incorporate True Shooting
ts_data <- read.csv("./nbacsv/TS.csv")  # Adjust file path

ts_data <- ts_data |> 
  rename(Name = Team) |>  # Rename 'Name' to 'Team'
  mutate(Name = gsub("\\*$", "", Name))  # Remove asterisks from 'Team' column

head(ts_data)
```


Now we join the three datasets on name and year
``` {r}
combined_data <- inner_join(combined_wins, playoff_data, by = c('Name', 'Year'))
combined_data <- inner_join(combined_data, ts_data, by = c('Name', 'Year'))
head(combined_data)
```

Because the RAPTOR dataset and the other datasets have different team names, we need to alter them to fit. The following is a dictionary for each team name. We also want to include conference, so we add that in too.
```{r}
#dataframe for team name, abbreviations, and conference
team_lookup <- data.frame(
  Team = c("San Antonio Spurs", "Oklahoma City Thunder", "Miami Heat", "Indiana Pacers",
           "Los Angeles Clippers", "Portland Trail Blazers", "Brooklyn Nets", "Washington Wizards",
           "Golden State Warriors", "Dallas Mavericks", "Memphis Grizzlies", "Toronto Raptors",
           "Houston Rockets", "Atlanta Hawks", "Chicago Bulls", "Charlotte Hornets",
           "Cleveland Cavaliers", "Boston Celtics", "Detroit Pistons", "Milwaukee Bucks",
           "New Orleans Pelicans", "Sacramento Kings", "Los Angeles Lakers", "Philadelphia 76ers",
           "Minnesota Timberwolves", "Utah Jazz", "Denver Nuggets", "New York Knicks", "Phoenix Suns", "Orlando Magic", "Charlotte Bobcats"),
  abbreviation = c("SAS", "OKC", "MIA", "IND", "LAC", "POR", "BKN", "WAS", "GSW", "DAL", "MEM",
                   "TOR", "HOU", "ATL", "CHI", "CHA", "CLE", "BOS", "DET", "MIL", "NOP", "SAC",
                   "LAL", "PHI", "MIN", "UTA", "DEN", "NYK", "PHO", "ORL", "CHB"), 
  Conference = c("West", "West", "East", "East", "West", "West", "East", "East", "West", "West", "West", "East", "West", "East", "East", "East", "East", "East", "East", "East", "West", "West", "West", "East", "West", "West", "West", "East", "West", "East", "East")
)
```

Performing join and renaming teams to abbreviations:
```{r}
#changing team name to abbreviation in win dataset
combined_data <- combined_data |>
  left_join(team_lookup, by = c("Name" = "Team")) |>
  mutate(Name = abbreviation) |>
  select(-abbreviation)

head(combined_data)

```

The BasketballReference work is done. Now, we can add the RAPTOR dataset. The first step is getting aggregate raptor score by team.
```{r}
raptor <- read_csv("modern_RAPTOR_by_team.csv")
by_team <- raptor |>
  group_by(season, team) |>
  summarise(composite_WAR = mean(war_reg_season)) |>
  rename(Year = season,
         Name = team)

head(by_team)
```

These values can't be used alone. Let's take a look at a particularly ergregious example:

``` {r}
filter(raptor, season==2014 & team == 'PHO')
```

This is one of the worst teams in the past 20 years, but have a very high average RAPTOR score. This underscores the importance of using multiple metrics for a situation as complex as this. Let's combine everything now.
``` {r}
combined_data <- inner_join(combined_data, by_team, by = c('Name', 'Year'))
head(combined_data)

```

# Data Manipulation
convert conference to boolean for use in KNN, remove team name

``` {r}
prepared_set <- combined_data |>
  mutate(conference_east = ifelse(Conference == 'East', T, F), # mutate conference to boolean
         Result = as.factor(Result)) |> # need to convert to factor for models
  select(Wins, Result, conference_east, composite_WAR, TS.)

head(prepared_set)
```

Now split the data into train and test sets
``` {r}
library(caTools) # For splitting the dataset
set.seed(5) # Set seed for reproducibility

split <- sample.split(prepared_set$Result, SplitRatio = 0.8) # Define the split ratio
train <- subset(prepared_set, split == TRUE) # Training set
test <- subset(prepared_set, split == FALSE) # Test set

head(train) # View the training set
head(test) # View the test set
```

# Model Training

```{r}
library(caret)
set.seed(123)
```

# Random Forest
```{r}
library(randomForest)

# Define parameters for Random Forest models
ntree_values <- c(5, 10, 15, 20, 50)
rf_grid <- expand.grid(mtry = 1:(ncol(train)-1))  # Tune 'mtry' parameter from 1 to 4
control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation

# Initialize a list to store results
results <- list()

# Loop over each ntree value, train Random Forest, and store results
for (ntree in ntree_values) {
  # Train the Random Forest model using caret with hyperparameter tuning
  rf_model <- train(
    Result ~.,            # Formula for target and predictor
    data = train,             # Training data
    method = "rf",            # Random Forest method
    tuneGrid = rf_grid,       # Hyperparameter tuning grid for 'mtry'
    trControl = control,      # Cross-validation setup
    ntree = ntree             # Number of trees in the forest
  )
  
  # Store the model's results in the list
  results[[paste("ntree", ntree, sep = "_")]] <- rf_model
}

# Extract and display the tuning results for each model
lapply(results, function(x) x$results)

```
Test it out!
```{r}
# Loop through each model in the results list and test on the test set
accuracy_results <- sapply(results, function(model) {
  # Make predictions on the test set
  predictions <- predict(model, newdata = test)
  
  # Compare predictions to actual values
  actual <- test$Result
  
  # Calculate the accuracy
  accuracy <- sum(predictions == actual) / length(actual)
  
  return(accuracy)
})

# Display the accuracy for each model
accuracy_results
```
ntree_5 and ntree_20 seem to have the highest accuracy, but ntree_5 is more simple. By Occam's razor, we'll use ntree_5 over ntree_20.

```{r}
rf_model_ntree_5 <- results[["ntree_5"]]

# Make predictions on the test set
rf_predictions <- predict(rf_model_ntree_5, newdata = test)

# Actual values from the test set
actual <- test$Result

# Generate the confusion matrix
confusion_matrix <- confusionMatrix(rf_predictions, test$Result)

# Extract the confusion matrix table
cm_table <- as.data.frame(confusion_matrix$table)

# Rename columns for clarity
colnames(cm_table) <- c("Actual", "Predicted", "Freq")

# Create the heat map
ggplot(data = cm_table, aes(x = Predicted, y = Actual, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(
    title = "Confusion Matrix Heat Map",
    x = "Predicted Label",
    y = "Actual Label",
    fill = "Frequency"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5)
  )
```

# KNN

Use a tuning grid to find the best performing model
```{r}
# Define the parameter grid for KNN
# 'k' is the number of neighbors to consider
knn_grid <- expand.grid(k = seq(3, 15, by = 2))  # Odd values of 'k' to avoid ties

# Define the cross-validation method
control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation

# Train the KNN model
knn_model <- train(
  Result ~.,                # Replace 'Wins' with relevant predictors
  data = train,                 # Training dataset
  method = "knn",               # Specify the KNN method
  tuneGrid = knn_grid,          # Grid of hyperparameters to tune
  trControl = control           # Cross-validation control
)

# View the results of the tuned model
print(knn_model)

# Best parameter (optimal 'k')
print(knn_model$bestTune)

# Evaluate the model's performance
knn_model_results <- knn_model$results
print(knn_model_results)

```

k=13 has the best performance. Let's now apply it to test set.

```{r}
# Make predictions on the test set
knn_predictions <- predict(knn_model, newdata = test)

# Generate the confusion matrix
confusion_matrix <- confusionMatrix(knn_predictions, test$Result)

# Extract the confusion matrix table
cm_table <- as.data.frame(confusion_matrix$table)

# Rename columns for clarity
colnames(cm_table) <- c("Actual", "Predicted", "Freq")

# Create the heat map
ggplot(data = cm_table, aes(x = Predicted, y = Actual, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(
    title = "Confusion Matrix Heat Map",
    x = "Predicted Label",
    y = "Actual Label",
    fill = "Frequency"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5)
  )
```
Histogram of class distribution
```{r}
rounds <- c('Round 1', 'Round 2', 'Conference Final', 'Final')

# Create the bar plot
ggplot(prepared_set) +
  geom_bar(fill = '#1D428A', aes(x = Result)) +
  scale_x_discrete(limits = rounds) +
  theme_minimal() +
  labs(
    title = 'Class Distribution of Playoff Results',
  )
```
