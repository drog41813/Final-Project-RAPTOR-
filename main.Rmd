---
title: "Utilizing RAPTOR Score to Predict NBA Playoff Performance"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
```

**Lan Do, Lilliana Forrest, Swara Kale, Daniel Lee**

## Introduction

This project leverages data from FiveThirtyEight’s RAPTOR score dataset, a modern metric designed to evaluate the efficacy of NBA players. Unlike older metrics such as CARMELO and DRAYMOND, RAPTOR incorporates advanced player-tracking and play-by-play data, emphasizing players' contributions to modern NBA schemes. Key attributes of this metric include its ability to account for shot creation, floor spacing, and defensive intensity, offering a more comprehensive evaluation of a player's impact on their team.

In addition to RAPTOR scores, the prepared dataset integrates team-level performance data, including overall season records, sourced from Basketball Reference. This combination of player- and team-level metrics allows for a detailed exploration of factors influencing playoff success, defined by the furthest round a team achieves in the postseason. 

The primary objective of this analysis is to develop predictive models to forecast playoff performance of NBA teams. This question is pertinent as much of the existing literature is concerned with predicting the outcome of an individual game - not postseason success. By addressing this question, the project aims to provide insights into the relationship between advanced player metrics and team success, with implications for team-building strategies and performance forecasting in professional basketball.

### Literature review of existing methods 

Matthew Houde created a baseline model where Gaussian Naïve Bayes was evaluated as the strongest model of predicting whether a team wins a given matchup. This is based on a variety of team aggregate stats including win percentage, field goal percentage, and 3-point shooting percentage (Houde). This yielded a 65% accuracy score.

Jake Kandell conducted a unique method where a z score for ELO was calculated for each team, then a logistic model was applied to the calculated z score and other statistics including win percentage, field goal percentage, and 3-point shooting percentage. This method yielded an astounding 70% accuracy. 

Although this method is effective, its implementation is incredibly sophisticated and outside the scope of the class. Thus, for the sake of simplicity, a z-score approach was not used for this analysis.

## Data Preparation

The RAPTOR dataset is retrieved from [538's RAPTOR dataset](https://github.com/fivethirtyeight/data/tree/master/nba-raptor). The remaining data was retrieved from BasketballReference, which is a database of various basketball data including wins, true shooting percentage, and post-season record. On Basketball Reference, we used the Total Stats and Advanced Stats datasets from the 2014-2022 seasons and cleared out all extraneous variables. On the Basketball Reference site, we were able to delete other variables such as free throw attempt and offensive rebounds, leaving the Total Stats dataset with only team name and rank, while the Advanced Stats dataset was left with team name and wins. The data for each year in Total Stats was added to a .txt file, which were all read into R. A year column was added to each dataset to keep track of what season was being referenced when the datasets per season were merged into one large dataset containing data from 2014 up to 2022. The same process was repeated for the Advanced Stats dataset, and the teams without asterisks next to their names were removed from the dataset as they did not continue to the regular season and would not be ranked in the top 16 teams. The asterisks were then removed from their names.

The datasets were then merged on the Name and Year variables. To change the names of the teams from full length to their three letter NBA abbreviations, a data frame was created that mapped the full length team names to their respective abbreviations. The season year was then added as a suffix to the team abbreviation to keep track of year more precisely and assist with joining to WAR data later on.

The WAR data from the 538 GitHub was opened in Excel, where all extraneous columns were deleted, leaving only team name in abbreviated form, WAR for the regular season, and year. The data was read into R, the team abbreviations received year as a suffix, and the WAR data was averaged by team abbreviation/year to get the team’s overall WAR in a season instead of each player’s specific WAR. 

An additional Numerical and Categorical variable was added to the dataset, specifically Conference of each team (East, West) and True shooting percentage for all playoff NBA teams. For conferences, a column called ‘Conference’ was created manually by adding it to the dataframe that contains both team names and abbreviations. For True shooting percentage, multiple datasets of NBA team’s true shooting percentage were utilized from the 2014-15 to 2021-22 seasons and then subsequently merged. 
Finally, the data from Advanced Stats, Total Stats, and the 538 GitHub were all merged on the Name_Year variable to produce a data table with the columns Name_Year, Results, Wins, and Average_WAR, Conference, and True shooting percentage for the top 16 teams during the 2014-2022 NBA seasons.

### Data Manipulation and Merging

We start by loading in all the playoff performance data, binding them together, and changing ranks to actual outcomes.  
```{r echo=FALSE}
#Reading in rank files
file_list <- c("data_2014.csv", "data_2015.csv", "data_2016.csv", "data_2017.csv", 
               "data_2018.csv", "data_2019.csv", "data_2020.csv", "data_2021.csv")
years <- c(2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021)  #years vector
modified_data_list <- list()
for (i in 1:length(file_list)) {
  data <- read.csv(file.path('./nbacsv', file_list[i]), header = FALSE)
  
  if(ncol(data) != 2) { # in case data is missing values
    print("Error detected in file" + file_list[i] + "!")
  }
  
  data$Year <- years[i]
  colnames(data) <- c("Result", "Name", "Year")
  modified_data_list[[i]] <- data
}
playoff_data <- do.call(rbind, modified_data_list)

# changing rank names
playoff_data <- playoff_data |>
  mutate(Result = case_when(
    Result == 1 | Result == 2 ~ "Final",
    Result == 3 | Result == 4 ~ "Conference Final",
    Result >= 5 & Result <= 8 ~ "Round 2",
    Result >= 9 & Result <= 16 ~ "Round 1"
  ))

head(playoff_data)
```
We then read in regular season wins.

```{r echo=FALSE}
# reading in win files
win_list <- c("wins_2014.csv", "wins_2015.csv", "wins_2016.csv", "wins_2017.csv", 
              "wins_2018.csv", "wins_2019.csv", "wins_2020.csv", "wins_2021.csv")
years <- c(2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021)
modified_win_list <- list()
for (i in 1:length(win_list)) {
  windata <- read.csv(file.path('nbacsv', win_list[i]), header = FALSE)
  if (ncol(windata) == 2) {
    windata$Year <- years[i]
    colnames(windata) <- c("Name", "Wins", "Year")
    modified_win_list[[i]] <- windata
  }
}

# combine files
combined_wins <- do.call(rbind, modified_win_list)
#removing wins that don't make it to playoffs, which are teams without an *
combined_wins <- combined_wins |>
  filter(grepl("\\*", Name)) |>
  mutate(Name = gsub("\\*$", "", Name))

head(combined_wins)
```

Finally, we read in true shooting percentages for each team.

```{r echo=FALSE}
# Incorporate True Shooting
ts_data <- read.csv("./nbacsv/TS.csv")  # Adjust file path

ts_data <- ts_data |> 
  rename(Name = Team) |>  # Rename 'Name' to 'Team'
  mutate(Name = gsub("\\*$", "", Name))  # Remove asterisks from 'Team' column

head(ts_data)
```

Now we join the three datasets on name and year
``` {r include=FALSE}
combined_data <- inner_join(combined_wins, playoff_data, by = c('Name', 'Year'))
combined_data <- inner_join(combined_data, ts_data, by = c('Name', 'Year'))
head(combined_data)
```

Because the RAPTOR dataset and the other datasets have different team names, we need to alter them to fit. The following is a dictionary for each team name. We also want to include conference, so we add that in too.
```{r echo=FALSE}
#dataframe for team name, abbreviations, and conference
team_lookup <- data.frame(
  Team = c("San Antonio Spurs", "Oklahoma City Thunder", "Miami Heat", "Indiana Pacers",
           "Los Angeles Clippers", "Portland Trail Blazers", "Brooklyn Nets", "Washington Wizards",
           "Golden State Warriors", "Dallas Mavericks", "Memphis Grizzlies", "Toronto Raptors",
           "Houston Rockets", "Atlanta Hawks", "Chicago Bulls", "Charlotte Hornets",
           "Cleveland Cavaliers", "Boston Celtics", "Detroit Pistons", "Milwaukee Bucks",
           "New Orleans Pelicans", "Sacramento Kings", "Los Angeles Lakers", "Philadelphia 76ers",
           "Minnesota Timberwolves", "Utah Jazz", "Denver Nuggets", "New York Knicks", "Phoenix Suns", "Orlando Magic", "Charlotte Bobcats"),
  abbreviation = c("SAS", "OKC", "MIA", "IND", "LAC", "POR", "BKN", "WAS", "GSW", "DAL", "MEM",
                   "TOR", "HOU", "ATL", "CHI", "CHA", "CLE", "BOS", "DET", "MIL", "NOP", "SAC",
                   "LAL", "PHI", "MIN", "UTA", "DEN", "NYK", "PHO", "ORL", "CHB"), 
  Conference = c("West", "West", "East", "East", "West", "West", "East", "East", "West", "West", "West", "East", "West", "East", "East", "East", "East", "East", "East", "East", "West", "West", "West", "East", "West", "West", "West", "East", "West", "East", "East")
)

head(team_lookup)
```

Performing join and renaming teams to abbreviations:
```{r echo=FALSE}
#changing team name to abbreviation in win dataset
combined_data <- combined_data |>
  left_join(team_lookup, by = c("Name" = "Team")) |>
  mutate(Name = abbreviation) |>
  select(-abbreviation)
head(combined_data)
```

The BasketballReference work is done. Now, we can add the RAPTOR dataset and get aggregate RAPTOR score by team.
```{r include=FALSE}
raptor <- read_csv("modern_RAPTOR_by_team.csv")
by_team <- raptor |>
  group_by(season, team, show_col_types = F) |>
  summarise(composite_WAR = mean(war_reg_season), show_col_types = FALSE) |>
  rename(Year = season,
         Name = team)
```

```{r}
head(by_team)
```

Now we combine everything by name and year.
``` {r echo=FALSE}
combined_data <- inner_join(combined_data, by_team, by = c('Name', 'Year'))
head(combined_data)
```

### Data Manipulation for Model Use
For ease of use in our models, we convert conference to a boolean, factor 'Result', and remove team name. We also split the dataset into a training and test set with a split factor of 0.8. 
``` {r echo=FALSE}
prepared_set <- combined_data |>
  mutate(conference_east = ifelse(Conference == 'East', T, F), # mutate conference to boolean
         Result = as.factor(Result)) |> # need to convert to factor for models
  select(Wins, Result, conference_east, composite_WAR, TS.)

head(prepared_set)
```

``` {r include=FALSE}
### Split into training and test sets!
library(caTools) # For splitting the dataset
set.seed(5) # Set seed for reproducibility

split <- sample.split(prepared_set$Result, SplitRatio = 0.8) # Define the split ratio
train <- subset(prepared_set, split == TRUE) # Training set
test <- subset(prepared_set, split == FALSE) # Test set

head(train) # View the training set
head(test) # View the test set
```

## Model Training and Validation
We chose to utilize a Random Forest and KNN model because they were some of the few classifier models we were familiar with. 

```{r include=FALSE}
library(caret)
set.seed(123)
```

### Random Forest
Random Forest is known to be robust on a wide range of datasets. We opted to optimize ntree in order to find the best-performing model. Given the relatively smaller size of our dataset, we opted to use smaller values for our tree and mtry.

```{r include=FALSE}
library(randomForest)
```

```{r include=FALSE}
# Define parameters for Random Forest models
ntree_values <- c(5, 10, 15, 20, 50)
rf_grid <- expand.grid(mtry = 1:(ncol(train)-1))  # Tune 'mtry' parameter from 1 to 4
control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation

# Initialize a list to store results
results <- list()

# Loop over each ntree value, train Random Forest, and store results
for (ntree in ntree_values) {
  # Train the Random Forest model using caret with hyperparameter tuning
  rf_model <- train(
    Result ~.,            # Formula for target and predictor
    data = train,             # Training data
    method = "rf",            # Random Forest method
    tuneGrid = rf_grid,       # Hyperparameter tuning grid for 'mtry'
    trControl = control,      # Cross-validation setup
    ntree = ntree             # Number of trees in the forest
  )
  
  # Adding ntree as a column to results data
  rf_model$results$ntree <- ntree
  
  # Store the model's results in the list
  results[[paste("ntree", ntree, sep = "_")]] <- rf_model
}

# Combine results into a single data frame
all_results <- do.call(rbind, lapply(results, function(model) model$results))

```

Here we see the results of the Random Forest model on the test set. 
```{r echo=FALSE}
# Find the row corresponding to the best-performing combination
best_result <- all_results[which.max(all_results$Accuracy), ]

# Display the best hyperparameters and performance
cat("Best Hyperparameters and Performance:\n")
print(best_result)
```
ntree_5 mtry 1 appears to have the best accuracy.

### Decision Tree

After viewing the results for the random forest, we decided to try a Decision Tree model because we found that our Random Forest results were below our expectations. Unlike Random Forest, the decision tree only uses one predictor at a time during each split whereas in a random forest model, it uses the random combination of our predictors. Our decision tree displays the percentage of teams that advance to each round.
```{r include=FALSE}
library(rpart)    # For decision tree
library(rpart.plot) # For decision tree visualization
# Train the decision tree on the training set
decision_tree_model <- rpart(
  Result ~ .,              # Formula for target and predictors
  data = train,            # Training dataset
  method = "class"         # Classification method
)
```
The following is a visualization of the tree. This result will be discussed further in the Results and Discussion Section.
```{r, echo = F, fig.align="center", fig.width=5, fig.height=3}
# Visualize the decision tree
print('Variable Importance:')
print(decision_tree_model$variable.importance)
rpart.plot(decision_tree_model)
```
<center>**Figure 1: **Decision Tree Diagram</center>

### KNN
KNN utilizes the similarity of data points to help predict playoff performance. We aimed to optimize k utilizng a grid search.

```{r echo=FALSE}
# Define the parameter grid for KNN
# 'k' is the number of neighbors to consider
knn_grid <- expand.grid(k = seq(3, 15, by = 2))  # Odd values of 'k' to avoid ties

# Define the cross-validation method
control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation

# Train the KNN model
knn_model <- train(
  Result ~.,                # Replace 'Wins' with relevant predictors
  data = train,                 # Training dataset
  method = "knn",               # Specify the KNN method
  tuneGrid = knn_grid,          # Grid of hyperparameters to tune
  trControl = control           # Cross-validation control
)
```

``` {r echo = FALSE}
# View the results of the tuned model
print(knn_model)

# Best parameter (optimal 'k')
print(knn_model$bestTune)

# Evaluate the model's performance
knn_model_results <- knn_model$results
```

k=5 has the best performance.

## Results and Discussion

The accuracy of the random forest model on the test set was 36.84% and the accuracy of the KNN model on the test set was also 36.84%. While the Random Forest model had somewhat higher accuracy than the KNN model, neither did very well overall. In comparison, previous models attempting to predict binary outcomes, such as whether a team will win or lose, achieved approximately 60% accuracy. Although these models are not directly comparable to our method, their increased accuracy highlights our models' weaknesses in this context.

The Random Forest performs with an accuracy of 36.84%. This indicates that the model did not generalize well. 

```{r echo=FALSE}
# Extract the full model for ntree = 5
rf_model_ntree_5 <- results[["ntree_5"]]

# Make predictions on the test set
rf_predictions <- predict(rf_model_ntree_5, newdata = test)

# Actual values from the test set
actual <- test$Result

# Get accuracy metrics
accuracy <- sum(rf_predictions == actual) / length(actual)
cat("Random Forest Test Set Accuracy:", accuracy)

```

The decision tree performed even worse than the Random Forest model. Examining the decision tree diagram in **Fig. 1** provides some insight as to the quality of our variables. The only somewhat reliable predictor of team performance was composite_WAR. This indicates that the variable choice had a significant detriment on the quality of the models.

```{r echo=FALSE, fig.align="center", fig.width=5, fig.height=3}
# Print the confusion matrix
# Generate the confusion matrix
confusion_matrix_RF <- confusionMatrix(rf_predictions, actual)

# Extract the confusion matrix table
cm_table_RF <- as.data.frame(confusion_matrix_RF$table)

# Rename columns for clarity
colnames(cm_table_RF) <- c("Actual", "Predicted", "Freq")

# Create the heat map
ggplot(data = cm_table_RF, aes(x = Predicted, y = Actual, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(
    title = "Random Forest Confusion Matrix Heat Map",
    x = "Predicted Label",
    y = "Actual Label",
    fill = "Frequency"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5)
  )
```

<center>**Figure 2: **Random Forest Confusion Matrix</center>

```{r, echo=FALSE}
# Predict on the test set
dt_predictions <- predict(decision_tree_model, newdata = test, type = "class")

# Get accuracy metrics
accuracy <- sum(dt_predictions == actual) / length(actual)
cat("Decision Tree Test Set Accuracy:", accuracy)
```

```{r, echo = FALSE, fig.align="center", fig.width=5, fig.height=3}
# Make confusion matrix
confusion_matrix_DT <- confusionMatrix(dt_predictions, actual)

# Extract the confusion matrix table
cm_table_DT <- as.data.frame(confusion_matrix_DT$table)

# Rename columns for clarity
colnames(cm_table_DT) <- c("Actual", "Predicted", "Freq")

# Create the heat map
ggplot(data = cm_table_DT, aes(x = Predicted, y = Actual, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(
    title = "Decision Tree Confusion Matrix Heat Map",
    x = "Predicted Label",
    y = "Actual Label",
    fill = "Frequency"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5)
  )
```

<center>**Figure 3: **Decision Tree Confusion Matrix</center>

The KNN performed as well as the Random Forest model.

```{r, echo=FALSE}
# Make predictions on the test set
knn_predictions <- predict(knn_model, newdata = test)

# Get accuracy metrics
accuracy <- sum(knn_predictions == actual) / length(actual)
cat("KNN Test Set Accuracy:", accuracy)

```

```{r, echo = FALSE, fig.align="center", fig.width=5, fig.height=3}
# Generate the confusion matrix
confusion_matrix_KNN <- confusionMatrix(knn_predictions, actual)

# Extract the confusion matrix table
cm_table_KNN <- as.data.frame(confusion_matrix_KNN$table)

# Rename columns for clarity
colnames(cm_table_KNN) <- c("Actual", "Predicted", "Freq")

# Create the heat map
ggplot(data = cm_table_KNN, aes(x = Predicted, y = Actual, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(
    title = "KNN Confusion Matrix Heat Map",
    x = "Predicted Label",
    y = "Actual Label",
    fill = "Frequency"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5)
  )
```

<center>**Figure 4: **KNN Confusion Matrix</center>

There are some key weaknesses of our analysis: size constraints of the dataset, class imbalance, model selection, and variable selection.

The size and constraints of the dataset, which only contained roughly 100 data points, had a substantial impact on the conclusions of our investigation. A relatively small sample size will inevitably cause the model to not train properly as the model will not have enough data to train on. Unfortunately, this is not something that can be directly fixed (e.g. by harvesting more data), as RAPTOR data is only available from 2014-2021 and is not currently maintained. 

The data's significant class imbalance presented a significant issue. Following each round of a given playoff season, half of teams are removed. **Fig. 5** clearly shows this trend. Because ‘Round 1’ teams are overrepresented, the model will poorly classify the minority classes (e.g. ‘finals’ teams). To fix this, techniques like oversampling the minority class or undersampling the majority class could be attempted in future iterations.

```{r echo=FALSE, fig.align="center", fig.width=5, fig.height=3}
rounds <- c('Round 1', 'Round 2', 'Conference Final', 'Final')

# Create the bar plot
ggplot(prepared_set) +
  geom_bar(fill = '#1D428A', aes(x = Result)) +
  scale_x_discrete(limits = rounds) +
  theme_minimal() +
  labs(
    title = 'Class Distribution of Playoff Results',
  )
```

<center>**Figure 5:** Histogram of class distribution</center>

Our models used categorical classifiers rather than ordinal categorical classifiers, which could be more representative of the NBA playoffs' evolution. The selection of models, such as Random Forest and KNN, may not have been ideal. Multinomial models or gradient boosting methods (GBMs) may have produced better results. 
Predicting NBA playoff outcomes is inherently complex and cannot be effectively reduced to just five variables. We also assumed that the RAPTOR data, a subjective assessment of player performance, was reliable, and that Average WAR accurately represented a team's overall success. However, these assumptions may not hold true in practice, thus introducing extra flaws into the analysis.

In conclusion, the chosen dataset suffered from its small size, there was significant class imbalance, the models selected used categorical instead of ordinal categorical classifiers, and the variables selected as predictors did not reflect the complexity of the NBA playoffs. The poor accuracy scores highlighted these shortcomings, demonstrating the models' inability to address our research question’s complexity. More robust datasets, alternate categorization methods, and other features may result in better performance.

## Reflection, Acknowledgement, and References
Coming to a consensus on selecting the ideal models that could use our data to produce the highest percentage of successful predictions was challenging. There wasn’t an ideal model that could achieve optimal accuracy for our results due to the amount of limitations that were present when running the RF/KNN tests, such as subpar alignment within the confusion matrices as well as being restricted to only 5 variables for the tests that were run. Extensive discussions and experimentation were required to explore various models such as logistic or linear regression and to strike a balance between the type of data we had, performance of the model, and feasibility. 

Through this project, we learned that predicting NBA playoff results, particularly with the datasets we utilized, was not easily possible by taking only five variables into account. Even with the addition of more related variables, such as steals or points, the models would never be able to paint an accurate picture of the result of a basketball game due to the sheer number of factors involved in the outcome, some of which could not be reproduced into data. For example, although it would certainly influence the outcome of the game, iit would be difficult to find or manually gather data on the mental state of every player before and during the match. This process emphasized the importance of robust data preparation and the importance of choosing the correct model based on the type of data we had gathered, teaching us how to navigate the challenges of working with complex datasets and imbalanced classifications.

The success of the project would not have been possible without the collaborative efforts of all team members. Swara was able to merge the datasets containing team names, years, Wins Above Replacement (WAR), regular season wins, and team rank spanning the 2014-2021 NBA seasons. She incorporated regular season wins data by joining it with the team rank dataset on abbreviations and year, creating a dataframe that matched each full team name to its respective abbreviation. She also wrote the data preparation section of the research report. 

Daniel added the conference for each team and their true shooting percentages to the dataset Swara created. To achieve this, Daniel mapped the conferences to each team within Swara’s abbreviation dataframe and joined datasets containing true shooting percentages for the respective seasons with Swara’s dataset. He contributed his additions to the data preparation section of the research report.

Lan primarily focused on creating the KNN model to interpret the results across the merged datasets and predict playoff performance. He wrote the KNN model and performed model testing to evaluate which provided the best performance in terms of predicting playoff outcomes. He also created a histogram that displayed the distribution of teams advancing through each round, from Round 1 to Finals, to emphasize how our models had less data to base predictions off of after every round of cuts within the playoff season. Lan wrote the introduction and the information related to the KNN model on the research report. Finally, Lan reviewed the code written by all team members to ensure efficiency and reproducibility.

Lilliana created the Random Forest model as an alternative to the KNN model and the Decision Tree model to create predictions on the playoff performance of the NBA teams. She also wrote the code for the Confusion matrices and heatmap associated with all the models to provide a visual representation of the inaccuracies our models contained in predicting the outcomes of NBA playoff season games.

<center> **References** </center>

Kandell, Jake. “JakeKandell/NBA-Predict.” GitHub, GitHub, 2019, github.com/JakeKandell/NBA-Predict.

Houde, Matthew. “Predicting the Outcome of NBA Games Predicting the Outcome of NBA Games”. Bryant Digital Repository, Bryant University, 2021. 

Basketball Reference: basketball-reference.com

RAPTOR score: https://github.com/fivethirtyeight/data/tree/master/nba-raptor